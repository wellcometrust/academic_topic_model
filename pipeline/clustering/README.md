# Dimensionality reduction and clustering

## UMAP

### Basic usage

To run dimensionality reduction with UMAP:
```
python3 pipeline/clustering/run_umap.py
```
This will generate 2-dimensional embeddings for a sample of data (located at `s3://datalabs-data/funding_impact_measures/embeddings_sample/`) using default parameters.

The following UMAP parameters are available from the command line:
* `--embedding_dim`, default=2
* `--n_neighbours`, default=15
* `--min_dist`, default=0.0
* `--metric`, default="euclidean"

Please refer to the official [UMAP documentation](https://https://umap-learn.readthedocs.io/en/latest/parameters.html) for guidance on parameter selection.

A basic plot can be generated by passing the flag `--save_plot` when creating 2-dimensional embeddings. UMAP embeddings are saved by appending them as a new column to the original text embeddings. The column name contains the number of dimensions and neighbours as a suffix (e.g., if you run UMAP with 5 dimensions and 20 neighbours, the column name will be "umap_5dim_20n").

For smaller datasets, you can also run UMAP from within a Jupyter notebook or Python script:
```python
from pipeline.clustering.clustering import UMAPEmbeddings

dimensionality_reduction = UMAPEmbeddings(
        input_bucket="s3://datalabs-data/funding_impact_measures/embeddings_sample/"
    )

dimensionality_reduction.run_umap(
    # pass in your UMAP parameters here
)
```

### Accelerate UMAP on large datasets

The following options have been implemented to help speed up the calculation of UMAP embeddings:

**GPU support**

You can run UMAP on a GPU with Nvidia Rapids [cuML](https://docs.rapids.ai/api/cuml/stable/). This has been implemented as part of the library - if you have cuML installed on your system and have a GPU available, UMAP will automatically run on the GPU using cuML. Please note that, as of now, there is no multi-GPU support and the embeddings need to fit on a single GPU.

**Precalculate k-nearest neighbours**

If the size of your embeddings dataset is too large (several million records), you may not be able to run UMAP from scratch on a single GPU (even if the size is less than your GPU memory, UMAP tends to crash if the dataset becomes too large). You can get around this by pre-computing the k-nearest neighbours and passing the indices and distances to UMAP, which can speed things up dramatically. Efficient knn computation has been implemented using the [Faiss](https://ai.meta.com/tools/faiss/) library. To run UMAP with pre-computed knn using Faiss from the command line:
```
python3 pipeline/clustering/run_umap.py --precalculate_knn
```
or:
```python
from pipeline.clustering.clustering import UMAPEmbeddings

dimensionality_reduction = UMAPEmbeddings(
        input_bucket="s3://datalabs-data/funding_impact_measures/embeddings_sample/" # s3 path to your embeddings
    )

dimensionality_reduction.run_umap(
    precalculate_knn=True,
    # pass in your other UMAP parameters here
)
```

To use Faiss on a CPU machine, you need to `pip install faiss-cpu`. By default, approximate knn distances and indices will be calculated using a Faiss IVFPQ index. Alternatively, you can attempt to run an exact knn calculation on a GPU if you `pip install faiss-gpu`. However, please note that faiss-gpu has several requirements which are currently incompatible with the rest of the pipeline.

## HDBSCAN

To run clustering with HDBSCAN:
```
python3 pipeline/clustering/run_hdbscan.py
```
This will generate cluster assignments for the same sample of data in the above s3 location.

The following HDBSCAN parameters are available from the command line:
* `--min_cluster_size`, default="50"
* `--min_samples`, default="50"
* `--epsilon`, default="0.0"

Please refer to the official [HDBSCAN documentation](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) for guidance on parameter selection.

You can also run HDBSCAN from within a Jupyter notebook or Python script, which allows you to pass in additional parameters from the full list as described in the official documentation:

```python
from pipeline.clustering.clustering import UMAPEmbeddings

hdbscan = HDBSCANClusters(
        input_bucket="s3://datalabs-data/funding_impact_measures/embeddings_sample/", # s3 path to your embeddings
        embedding_col="umap_5dim_15n" # column containing UMAP embeddings
    )

hdbscan.run_hdbscan(
    # pass in your HDBSCAN parameters here
)
```

### Parameter selection
It can be useful to try a range of parameters for HDBSCAN. A random parameter search has been implemented, which can be run by passing several (comma-separated) HDBSCAN parameter values on the command line and specifying the number of trials with `--n_params`:

```
python3 pipeline/clustering/run_hdbscan.py --min_cluster_size="50,100,500" --min_samples="25,50,100,200" --n_params=10
```
The following metrics will be calculated for each run:
* selected HDBSCAN parameters
* validity score
* total number of clusters
* coverage (proportion of non-outlier points)

The full results will be logged to Weights & Biases, if the --wandb flag is set to true. Follow these [simple instructions](https://wandb.wellcome-data.org/quickstart?utm_source=app-resource-center&utm_medium=app&utm_term=quickstart) (1. Set up the wandb library) to login to Weights & Biases. Alternatively, the results will be saved to csv (hdbscan_gridsearch.csv).
